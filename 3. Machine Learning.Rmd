---
title: "Master's Thesis Code - Machine Learning"
author: "Ana Pérez"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## PREPARING THE DATA FOR PREDICTION

```{r,eval=FALSE}
sum_emp_isco <-merged_data_subset %>%
  group_by(YEAR, isco) %>%
  summarise(sum_isco = sum(totalemp, na.rm = TRUE)) 

sum_emp_year <- merged_data_subset %>%
  group_by(YEAR) %>%
  summarise(sum_year = sum(totalemp, na.rm = TRUE))

merged_totals <- sum_emp_isco%>%
  left_join(sum_emp_year, by = c("YEAR")) 

merged_totals <- merged_totals %>%
  mutate(share = (sum_isco / sum_year) * 100) |> dplyr::select(YEAR,isco,share)

 merged_totals <- merged_totals |> 
  arrange(isco, YEAR) %>% 
   group_by(isco) |> # Ordenar por isco y YEAR para asegurar el orden correcto
  mutate(cambio = share - lag(share),  # Calcular el cambio
         cambio = ifelse(is.na(cambio), 0, round(cambio, 2))) 

merged_data_subset<- merged_data_subset%>%
  left_join(merged_totals, by = c("YEAR","isco"))
```


```{r}
library(lme4)
library(lmerTest)
library(modelsummary) 
library(broom)
library(knitr)
```

### NA imputation

```{r}
new_data <- merged_data_subset |> 
  dplyr::select(-job,-sector,-occup,-totalemp,-empchange,-avg_change) |> 
  distinct(.keep_all = TRUE)
  
sapply(new_data, function(x) sum(is.na(x))*100/nrow(new_data))
```

We have to make sure there are no NA'S in our data. For that purpose, and since the number of NA is very low, we are using the...imputations with the Random Forest algorithm from the mice function. 

```{r}
set.seed(123)
m = 5
mice_mod <- mice(new_data, m=m, method='rf')
new_data <- complete(mice_mod, action=m)
```

### Distribution of target variable

This is the distribution of the outcome variable: 

```{r}
new_data %>% 
  ggplot(aes(x=iscoprop)) +
  geom_density(fill="navyblue") + theme_minimal()
```

```{r}
training <- subset(new_data, YEAR >= 2012 & YEAR <= 2018) 
testing<- subset(new_data, YEAR ==2019) 
```

### Correlations

```{r}
cor_data <-training |> dplyr::select(-COUNTRY,-YEAR,-isco)
corr_change <- sort(cor(cor_data)["iscoprop",], decreasing = TRUE)
corr <- data.frame(Predictor = names(corr_change), Correlation = corr_change)
```

```{r}
nudge_x <- 0.01
my_col <- viridisLite::mako(4)[2]

corr %>% 
  ggplot(aes(y = reorder(Predictor, Correlation), x = Correlation)) +
  geom_point(col = my_col) +
  geom_segment(aes(xend = 0, yend = Predictor), col = my_col, size = 0.75) +
  geom_text(
    aes(x = 0, label = Predictor),
    size = 2.5,
    hjust = if_else(corr$Correlation > 0, 1, 0),
    nudge_x = if_else(corr$Correlation > 0, -nudge_x, nudge_x)
  ) +
  scale_y_discrete(breaks = NULL) +
  labs(title = "Correlations with the target variable",
       x = '', y = element_blank()) + theme_minimal()
```

### REGRESSION METHODS

#### Lambda coefficients

```{r}
set.seed(123)
# Use non-numeric
X <- cor_data |> select (-iscoprop)
Y <- cor_data |> select (iscoprop)

grid = 10^seq(10, -2, length = 100)

lasso_data<-cv.glmnet(x=as.matrix(X), y=Y[,1], lambda=grid, alpha=1)
lambda_min<-predict(lasso_data, newx=as.matrix(X)[1:10,], s="lambda.min")
lasso_coeff <- as.data.frame(as.matrix(predict(lasso_data, type="coefficients", s="lambda.min"))) |>  mutate(across(c(lambda.min), round, 2))

coeff_df <- as.data.frame(as.matrix(lasso_coeff))
coeff_df$variable <- rownames(coeff_df)

# Filter non-zero coefficients and exclude intercept
selected_variables <- coeff_df %>%
  filter(lambda.min != 0) %>%
  pull(variable) %>%
  setdiff("(Intercept)")

print(selected_variables)
```

```{r}
all_variables <- c(selected_variables, "as.factor(COUNTRY)", "YEAR")
# Subset the original data to include only the selected variables
formula <- as.formula(paste("iscoprop ~ ", paste(all_variables, collapse = " + ")))
selected_data <- training %>% select(all_of(c("iscoprop", selected_variables, "COUNTRY","YEAR")))
```


#### Ordinary Least Squares (OLS)

```{r}
set.seed(123)
# Fit a linear model
r1 <- lm(formula, data = selected_data)
summary(r1)
```

#### Linear Mixed Models

```{r}
formula
```

```{r}
set.seed(123)
r2 <- lmer(iscoprop ~ sunratio + satratio + nightratio + shiftratio + extraratio + 
    supratio + ftratio + onratio + femratio + strength + dexterity + 
    navigation + uncodified + business + humanities + numeracy + 
    infogath + creativity + planning + social + serving + managing + 
    autonomy + latitude + control + team + repetitiv + standard + 
    certainty + machines + ICT + ICTbasic + ICTadvanced  + 
      (1 | COUNTRY) +  (1 | YEAR), data = training)

summary(r2)
```

```{r}
r3 <- lmer(iscoprop ~  strength + dexterity + 
    navigation + uncodified + business + humanities + numeracy + 
    infogath + creativity + planning + social + serving + managing + 
    autonomy + latitude + control + team + repetitiv + standard + 
    certainty + machines + ICT + ICTbasic + ICTadvanced +
           (1 + sunratio + nightratio + shiftratio +
    supratio + femratio | COUNTRY) + 
           (1 | YEAR), 
          control = lmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 10000)), 
              data = training)
summary(r3)
```

```{r}
library(lmerTest)
library(sjPlot)
library(kableExtra)
```

### Evaluation and Robustness

From now on, we are going to try many models, so it’s convenient to create a data frame with all the predictors.

```{r}
test_results <- data.frame(iscoprop = testing$iscoprop)
```

```{r}
# Linear model 
test_results$lm <- predict(r1, newdata = testing, allow.new.levels = TRUE)
lm<- postResample(pred = test_results$lm,  obs = test_results$iscoprop)

# Linear Mixed Model (COUNTRY)
test_results$lmm <- predict(r2, newdata = testing, allow.new.levels = TRUE)
lmm<-postResample(pred = test_results$lmm,  obs = test_results$iscoprop)

# Linear Mixed Model (COUNTRY, YEAR,ISCO)
test_results$lmm2 <- predict(r3, newdata = testing, allow.new.levels = TRUE)
lmm2<-postResample(pred = test_results$lmm2,  obs = test_results$iscoprop)
```

```{r}
metrics_table <- data.frame(
  Model = c("OLS", "Simple LMM", "Sophisticated LMM"),
  RMSE = c(lm[1], lmm[1], lmm2[1]),
  R2 = c(lm[2], lmm[2], lmm2[2]),
  MAE = c(lm[3], lmm[3], lmm2[3])
)
metrics_table
```

```{r}
qplot(test_results$lmm2, test_results$iscoprop) + 
  labs(title="Linear Regression Observed VS Predicted",
       x="Predicted", y="Observed") +
  lims(x = c(0,20), y = c(0,20)) +
  geom_abline(intercept = 0, slope = 1, colour = "darkred") +
  theme_bw()
```

Extracting the proportion of variance due to COUNTRY/YEAR random effects

```{r}
# Extract variance components
var_components <- VarCorr(r3)
print(var_components)

# Extract random effect variance for COUNTRY
random_effect_variance <- unlist(var_components$COUNTRY)[1] 
# Total variance (residual + random effect variance)
total_variance <- attr(VarCorr(r3), "sc")^2

# Proportion of variance due to random effect 
prop_variance_random <- random_effect_variance / total_variance
prop_variance_random
```

```{r}
coefs <- summary(r3)$coefficients

coefs_df <- data.frame(Variable = rownames(coefs), Estimate = coefs[, 1], CI_low = coefs[, 1] - 1.96 * coefs[, 2], CI_high = coefs[, 1] + 1.96 * coefs[, 2])

ggplot(coefs_df, aes(x = reorder(Variable, Estimate), y = Estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = CI_low, ymax = CI_high), width = 0.2) +
  coord_flip() +
  theme_minimal() 
```

Let’s summarize the MAE for all the tools to select the best models in predicting our response variable:

```{r}
diff_rounded <- apply(test_results[-1], 2, function(x) round(mean(abs(x - test_results$iscoprop)), digits = 8))
diff_rounded
```

```{r}
# Combination
test_results$pred_change = test_results$lmm2
postResample(pred = test_results$pred_change,  obs = test_results$iscoprop)
```

```{r}
yhat = test_results$pred_change
hist(yhat, col="lightblue")
```

**Comparison of real vs predicted average change in employment levels across occupations**

```{r}
real_change <- test_results %>% ggplot(aes(x=iscoprop)) + 
  geom_density(fill="navyblue") + xlim(0, 20) 
pred_change<- test_results |> ggplot(aes(x=pred_change)) + 
  geom_density(fill="navyblue") +  xlim(0, 20)

# Arrange together
ggarrange(real_change,pred_change,
          ncol =2, nrow=1) 
```
```{r}
library(sjPlot)
#plot_model(model3, type = "pred")

fixed_effects <- fixef(r3)
importance <- abs(fixed_effects[-1])  # Exclude the intercept
importance_df <- data.frame(Variable = names(importance), Importance = importance)
importance_df <- importance_df %>% arrange(desc(Importance))

plot <- plot_model(r3, type = "est", show.values = TRUE, value.offset = .3) +
  labs(title = "Fixed Effects - model3") + theme_minimal()
  
plot +
  scale_x_discrete(limits = importance_df$Variable) + 
  labs(title = "Fixed Effects - Linear Mixed Model")
```
```{r}
stargazer(r1)
```





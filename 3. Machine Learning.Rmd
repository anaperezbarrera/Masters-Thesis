---
title: "Master's Thesis Code - Statistical Analysis"
author: "Ana Pérez"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r,eval=FALSE}
sum_emp_isco <-merged_data_subset %>%
  group_by(YEAR, sector) %>%
  summarise(sum_isco = sum(totalemp, na.rm = TRUE)) 

sum_emp_year <- merged_data_subset %>%
  group_by(YEAR) %>%
  summarise(sum_year = sum(totalemp, na.rm = TRUE))

merged_totals <- sum_emp_isco%>%
  left_join(sum_emp_year, by = c("YEAR")) 

merged_totals <- merged_totals %>%
  mutate(share = (sum_isco / sum_year) * 100) |> dplyr::select(YEAR,sector,share)
```

```{r}
merged_totals <- merged_totals |> 
  arrange(sector, YEAR) %>% 
   group_by(sector) |> 
  mutate(cambio = share - lag(share),
cambio = ifelse(is.na(cambio), 0, round(cambio, 2)))


merged_data_subset<- merged_data_subset%>%
  left_join(merged_totals, by = c("YEAR","sector"))
```


## PREPARING THE DATA 

```{r}
library(lme4)
library(lmerTest)
library(modelsummary) 
library(broom)
library(knitr)
library(sjPlot)
library(kableExtra)
```

### 1) Proportion of employees 

### NA imputation

```{r,eval=FALSE}
new_data <- merged_data_subset |> 
  dplyr::select(-job,-sector,-totalemp,-iscoprop,-empchange,-avg_change,-share) |> 
  distinct(.keep_all = TRUE)
  
sapply(new_data, function(x) sum(is.na(x))*100/nrow(new_data))
```

We have to make sure there are no NA'S in our data. For that purpose, and since the number of NA is very low, we are using the...imputations with the Random Forest algorithm from the mice function. 

```{r,eval=FALSE}
set.seed(123)
m = 5
mice_mod <- mice(new_data, m=m, method='rf')
new_data <- complete(mice_mod, action=m)
```

### Distribution of target variable

This is the distribution of the outcome variable: 

```{r}
new_data %>% 
  ggplot(aes(x=cambio)) +
  geom_density(fill="navyblue") + theme_minimal()
```

```{r}
training <- subset(new_data, YEAR >= 2012 & YEAR <= 2018) 
testing<- subset(new_data, YEAR == 2019) 
```

### Correlations

```{r}
cor_data <-training |> dplyr::select(-COUNTRY,-YEAR,-isco)
corr_change <- sort(cor(cor_data)["cambio",], decreasing = TRUE)
corr <- data.frame(Predictor = names(corr_change), Correlation = corr_change)
```

```{r}
nudge_x <- 0.01
my_col <- viridisLite::mako(4)[2]

corr %>% 
  ggplot(aes(y = reorder(Predictor, Correlation), x = Correlation)) +
  geom_point(col = my_col) +
  geom_segment(aes(xend = 0, yend = Predictor), col = my_col, size = 0.75) +
  geom_text(
    aes(x = 0, label = Predictor),
    size = 2.5,
    hjust = if_else(corr$Correlation > 0, 1, 0),
    nudge_x = if_else(corr$Correlation > 0, -nudge_x, nudge_x)
  ) +
  scale_y_discrete(breaks = NULL) +
  labs(title = "Correlations with the target variable",
       x = '', y = element_blank()) + theme_minimal()
```

## REGRESSION METHODS

### Lambda coefficients

```{r}
set.seed(123)
# Use non-numeric
X <- cor_data |> select (-cambio)
Y <- cor_data |> select (cambio)

grid = 10^seq(10, -2, length = 100)

lasso_data<-cv.glmnet(x=as.matrix(X), y=Y[,1], lambda=grid, alpha=1)
lambda_min<-predict(lasso_data, newx=as.matrix(X)[1:10,], s="lambda.min")
lasso_coeff <- as.data.frame(as.matrix(predict(lasso_data, type="coefficients", s="lambda.min"))) |>  mutate(across(c(lambda.min), round, 2))

coeff_df <- as.data.frame(as.matrix(lasso_coeff))
coeff_df$variable <- rownames(coeff_df)

# Filter non-zero coefficients and exclude intercept
selected_variables <- coeff_df %>%
  filter(lambda.min != 0) %>%
  pull(variable) %>%
  setdiff("(Intercept)")

print(selected_variables)
```
### Ordinary Least Squares (OLS)

```{r}
# Subset the original data to include only the selected variables
formula <- as.formula(paste("cambio ~ ", paste(selected_variables, collapse = " + ")))
```

```{r}
set.seed(123)
# Fit a linear model
lm <- lm(formula, data=training)
summary(lm)
```
### OLS controlling for country fixed-effects

```{r}
all_variables <- c(selected_variables, "as.factor(COUNTRY)", "YEAR")
formula <- as.formula(paste("cambio ~ ", paste(all_variables, collapse = " + ")))
selected_data <- training %>% select(all_of(c("cambio", selected_variables, "COUNTRY","YEAR")))
```


```{r}
set.seed(123)
# Fit a linear model
lm2 <- lm(formula, 
    data = training)
summary(lm2)
```
```{r}
all_variables <- c(selected_variables, "as.factor(COUNTRY)", "as.factor(YEAR)")
formula <- as.formula(paste("cambio ~ ", paste(all_variables, collapse = " + ")))
selected_data <- training %>% select(all_of(c("cambio", selected_variables, "COUNTRY","YEAR", "ICTadvanced")))
```

```{r}
# Fit a linear model
lm3 <- lm(formula, 
    data = training)
summary(lm3)
```

### Linear Mixed Models

```{r}
set.seed(123)
lmm <- lmer(cambio ~ femratio*ICTadvanced + onratio*ICTadvanced + femratio*machines + 
              serving + caring + repetitiv + machines  + ICTadvanced + (1 | YEAR), 
             control = lmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 10000)), data = training)
           

summary(lmm)
```

```{r}
lmm2 <- lmer(cambio ~  serving + caring + repetitiv +machines + ICTadvanced + 
               (1 + femratio | YEAR), 
             control = lmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 10000)), data = training)

summary(lmm2)
```

```{r}
coefs <- summary(lmm)$coefficients
coefs_df <- data.frame(Variable = rownames(coefs), 
                       Estimate = coefs[, 1], CI_low = coefs[, 1] - 1.96 * coefs[, 2], 
                       CI_high = coefs[, 1] + 1.96 * coefs[, 2])

ggplot(coefs_df, aes(x = reorder(Variable, Estimate), y = Estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = CI_low, ymax = CI_high), width = 0.2) +
  coord_flip() +
  theme_minimal() 
```

#### Fixed-effects

```{r}
fixed_effects <- fixef(lmm2)
importance <- abs(fixed_effects[-1])  # Exclude the intercept
importance_df <- data.frame(Variable = names(importance), Importance = importance)
importance_df <- importance_df %>% arrange(desc(Importance))

top_effects <- importance_df$Variable[1:5]

plot <- plot_model(lmm2, type = "est", show.values = TRUE, value.offset = .3) +
  labs(title = "") + theme_minimal()
  
plot +
  scale_x_discrete(limits = top_effects) + labs(x="",y="",title = "") 
```

#### Random-effects

### EVALUATION AND ROBUSTNESS

From now on, we are going to try many models, so it’s convenient to create a data frame with all the predictors.

```{r}
test_results <- data.frame(cambio = testing$cambio)
```

```{r}
# Linear model 
test_results$ols <- predict(lm, newdata = testing, allow.new.levels = TRUE)
lm<- postResample(pred = test_results$ols,  obs = test_results$cambio)

test_results$ols2 <- predict(lm2, newdata = testing, allow.new.levels = TRUE)
lm2<- postResample(pred = test_results$ols2,  obs = test_results$cambio)

# Linear Mixed Model 
test_results$lmm <- predict(lmm, newdata = testing, allow.new.levels = TRUE)
lmm<-postResample(pred = test_results$lmm,  obs = test_results$cambio)

test_results$lmm2 <- predict(lmm2, newdata = testing, allow.new.levels = TRUE)
lmm2<-postResample(pred = test_results$lmm2,  obs = test_results$cambio)
```

```{r}
metrics_table <- data.frame(
  Model = c("OLS (1)","OLS(2)", "LMM (1)", "LMM (2)"),
  RMSE = c(lm[1], lm2[1],lmm[1], lmm2[1]),
  R2 = c(lm[2], lm2[2],lmm[2], lmm2[2]),
  MAE = c(lm[3], lm2[3],lmm[3], lmm2[3])
)
metrics_table
```

```{r}
qplot(test_results$lmm, test_results$cambio) + 
  labs(title="Linear Regression Observed VS Predicted",
       x="Predicted", y="Observed") +
  geom_abline(intercept = 0, slope = 1, colour = "darkred") +
  theme_bw()
```


Let’s summarize the MAE for all the tools to select the best models in predicting our response variable:

```{r}
diff_rounded <- apply(test_results[-1], 2, function(x) round(mean(abs(x - test_results$cambio)), 
                                                             digits = 8))
diff_rounded
```


```{r}
library(lme4)
library(caret)

# Create folds
set.seed(123)
folds <- createFolds(training$cambio, k = 10)

# Initialize storage for results
results <- list()

for(i in 1:length(folds)) {
  training_indices <- setdiff(1:nrow(training), folds[[i]])
  training_fold <- training[training_indices, ]
  validation_fold <- training[folds[[i]], ]
  
  # Train the model
  model <- lmer(cambio ~ serving + caring + repetitiv + machines + ICTadvanced + 
                (1 + femratio | YEAR),  
                control = lmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 10000)), 
                data = training_fold)
  
  # Predict on the validation set
  predictions <- predict(model, newdata = validation_fold, allow.new.levels = TRUE)
  
  # Store results for each fold
  fold_results <- data.frame(
    obs = validation_fold$cambio,
    pred = predictions
  )
  results[[i]] <- fold_results
}

# Combine results from all folds into a single data frame
all_results <- do.call(rbind, results)

# Calculate performance metrics
mae <- mean(abs(all_results$obs - all_results$pred))
mse <- mean((all_results$obs - all_results$pred)^2)
rmse <- sqrt(mse)
r_squared <- cor(all_results$obs, all_results$pred)^2

performance_df <- data.frame(
  MAE = mae,
  MSE = mse,
  RMSE = rmse,
  R_squared = r_squared
)

# Print or use the dataframe as needed
performance_df

```

**Comparison of real vs predicted average change in employment levels across occupations**

```{r}
real_change <- all_results %>% ggplot(aes(x=obs)) + 
  geom_density(fill="navyblue")  +theme_minimal()
pred_change<- all_results |> ggplot(aes(x=pred)) + 
  geom_density(fill="navyblue")  + theme_minimal()

# Arrange together
ggarrange(real_change,pred_change,
          ncol =2, nrow=1) 
```

```{r}
library(texreg)
# Usar stargazer para comparar modelos
texreg(list(lmm, lmm2), file = "output.tex")
```


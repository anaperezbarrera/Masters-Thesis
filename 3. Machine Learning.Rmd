---
title: "Machine Learning"
author: "Ana Pérez"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## PREPARING THE DATA FOR PREDICTION

```{r, eval=FALSE}
library(openxlsx)
write.xlsx(merged_data_subset, file = "merged_data.xlsx", rowNames = FALSE)
```

### NA imputation

```{r}
new_data <- merged_data_subset |> 
  dplyr::select(-isco,-EMPSTAT,-sector, -empchange, -avg_change,-avg_year) |> 
  distinct(.keep_all = TRUE)
  
sapply(new_data, function(x) sum(is.na(x))*100/nrow(new_data))
```


```{r}
set.seed(123)
m = 5
mice_mod <- mice(new_data, m=m, method='rf')
new_data <- complete(mice_mod, action=m)
```

```{r}
training <- subset(new_data, YEAR >= 2012 & YEAR <= 2019) 
testing<- subset(new_data, YEAR ==2020) 
```

### Distribution of target variable

This is the distribution of the outcome variable: change of employment levels across the 40 occupations in the last 10 years.

```{r}
training %>% 
  ggplot(aes(x=iscoprop)) +
  geom_density(fill="navyblue") + theme_minimal()
```


```{r}
library(viridis)
library(RColorBrewer)

ggplot(merged_data_subset, aes(x = reorder(as.factor(sector),avg_change),  y = avg_change)) +
 geom_boxplot(outlier.colour = "red", outlier.shape = 16, notch = TRUE, notchwidth = 0.5) +
  theme_minimal() +
  coord_flip()+
  labs(x = "",
       y = "") + theme(legend.position = "none")
```

### Correlations

```{r}
cor_data <-training |> dplyr::select(-COUNTRY,-YEAR)
corr_change <- sort(cor(cor_data)["iscoprop",], decreasing = TRUE)
corr <- data.frame(Predictor = names(corr_change), Correlation = corr_change)
```

```{r}
nudge_x <- 0.01
my_col <- viridisLite::mako(3)[2]

corr %>% 
  ggplot(aes(y = reorder(Predictor, Correlation), x = Correlation)) +
  geom_point(col = my_col) +
  geom_segment(aes(xend = 0, yend = Predictor), col = my_col, size = 0.75) +
  geom_text(
    aes(x = 0, label = Predictor),
    size = 2.5,
    hjust = if_else(corr$Correlation > 0, 1, 0),
    nudge_x = if_else(corr$Correlation > 0, -nudge_x, nudge_x)
  ) +
  scale_y_discrete(breaks = NULL) +
  labs(title = "Correlations with the target variable",
       x = 'Correlation', y = element_blank()) + theme_minimal()
```


### Looking for the best lambda coefficient

```{r}
set.seed(123)
X <- training |> select (-iscoprop)
Y <- training |> select (iscoprop)

grid = 10^seq(10, -2, length = 100)

lasso_data<-cv.glmnet(x=as.matrix(X), y=Y[,1], lambda=grid, alpha=1)
lambda_min<-predict(lasso_data, newx=as.matrix(X)[1:10,], s="lambda.min")
lasso_coeff <- as.data.frame(as.matrix(predict(lasso_data, type="coefficients", s="lambda.min"))) |>  mutate(across(c(lambda.min), round, 2))

coeff_df <- as.data.frame(as.matrix(lasso_coeff))
coeff_df$variable <- rownames(coeff_df)

# Filter non-zero coefficients and exclude intercept
selected_variables <- coeff_df %>%
  filter(lambda.min != 0) %>%
  pull(variable) %>%
  setdiff("(Intercept)")

print(selected_variables)
```

```{r}
all_variables <- c(selected_variables, "as.factor(COUNTRY)", "YEAR")
# Subset the original data to include only the selected variables
formula <- as.formula(paste("iscoprop ~ ", paste(all_variables, collapse = " + ")))
selected_data <- training %>% select(all_of(c("iscoprop", selected_variables, "COUNTRY", "YEAR")))
formula
```

```{r}
set.seed(123)
# Fit a linear model
lm_model <- lm(formula, data = selected_data)
summary(lm_model)
```

## LINEAR MIXED MODEL

```{r}
set.seed(123)
library(lme4)
# Linear mixed model
lmm_model <- lmer(iscoprop ~ sunratio + satratio + nightratio + shiftratio + extraratio + supratio + ftratio + onratio + femratio + strength + dexterity + navigation + uncodified + business + humanities + numeracy + infogath + creativity + planning + social + serving + managing + autonomy + latitude + control + team + repetitiv + standard + certainty + machines + ICT + ICTbasic + ICTadvanced +  (1 | COUNTRY), data = training)
summary(lmm_model)
```

```{r}
model3 <- lmer(iscoprop ~ sunratio + satratio + nightratio + shiftratio + extraratio + supratio + ftratio + onratio + femratio +  strength + dexterity + navigation + uncodified + business + humanities + numeracy + infogath + creativity +  planning + social + serving + managing + autonomy + latitude + control + team + repetitiv + standard + certainty + machines + ICT + ICTbasic + ICTadvanced +
           (1 | COUNTRY) + 
           (1 | job) + 
           (1 | YEAR), 
          control = lmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 100000)),
              data = training)
summary(model3)
```

From now on, we are going to try many models, so it’s convenient to create a data frame with all the predictors.

```{r}
test_results <- data.frame(iscoprop = testing$iscoprop)
```

```{r}
# Linear model 
test_results$lm <- predict(lm_model, newdata = testing, allow.new.levels = TRUE)
postResample(pred = test_results$lm,  obs = test_results$iscoprop)

# Linear Mixed Model (COUNTRY)
test_results$lmm <- predict(lmm_model, newdata = testing, allow.new.levels = TRUE)
postResample(pred = test_results$lmm,  obs = test_results$iscoprop)

# Linear Mixed Model (COUNTRY, YEAR,ISCO)
test_results$lmm2 <- predict(model3, newdata = testing, allow.new.levels = TRUE)
postResample(pred = test_results$lmm2,  obs = test_results$iscoprop)

```


```{r}
lm_coef <- varImp(lm_model, scale = FALSE)
```

Extracting the proportion of variance due to COUNTRY/YEAR random effects

```{r}
# Extract variance components
var_components <- VarCorr(lmm_model)
print(var_components)

# Extract random effect variance for COUNTRY
random_effect_variance <- unlist(var_components$COUNTRY)[1] 
# Total variance (residual + random effect variance)
total_variance <- attr(VarCorr(lmm_model), "sc")^2

# Proportion of variance due to random effect (COUNTRY)
prop_variance_random <- random_effect_variance / total_variance
prop_variance_random
```


```{r}
coefs <- summary(model3)$coefficients
coefs_df <- data.frame(Variable = rownames(coefs), Estimate = coefs[, 1], CI_low = coefs[, 1] - 1.96 * coefs[, 2], CI_high = coefs[, 1] + 1.96 * coefs[, 2])

ggplot(coefs_df, aes(x = reorder(Variable, Estimate), y = Estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = CI_low, ymax = CI_high), width = 0.2) +
  coord_flip() +
  theme_minimal() 
```


## ADVANCED REGRESSION


#### The train control

```{r}
set.seed(123)
ctrl <- trainControl(method = "repeatedcv", 
                     number = 5, repeats = 1)
```

### Linear regression

```{r}
set.seed(123)
lm_tune <- train(formula,  
                 data = training, 
                 method = "lm", 
                 preProc = c('scale', 'center'),
                 trControl = ctrl)
lm_tune

test_results$lm <- predict(lm_tune, newdata=testing)
postResample(pred = test_results$lm,  obs = test_results$iscoprop)
```

```{r}
qplot(test_results$lm, test_results$iscoprop) + 
  labs(title="Linear Regression Observed VS Predicted",
       x="Predicted", y="Observed") +
  lims(x = c(-1,1), y = c(-1,1)) +
  geom_abline(intercept = 0, slope = 1, colour = "darkred") +
  theme_bw()
```

### Overfitted linear regression

```{r}
set.seed(123)
alm_tune <- train(formula, data = training, 
                  method = "lm", 
                  preProc=c('scale', 'center'),
                  trControl = ctrl)
alm_tune

test_results$alm <- predict(alm_tune, testing)
postResample(pred = test_results$alm,  obs = test_results$iscoprop)
```

### Forward Regression

```{r}
set.seed(123)
for_tune <- train(formula, 
                  data = training, 
                  method = "leapForward", 
                  preProc=c('scale', 'center'),
                 tuneGrid = expand.grid(nvmax = 1:10),
                  trControl = ctrl)

for_tune
```

```{r}
set.seed(123)
test_results$frw <- round(predict(for_tune, testing),8)
postResample(pred = test_results$frw,  obs = test_results$iscoprop)
```

```{r}
coef(for_tune$finalModel, for_tune$bestTune$nvmax)
```

These coefficients represent the estimated effects of each predictor variable on the response variable in the forward regression model with the optimal number of predictors.

```{r}
 formula2 = iscoprop ~ supratio+ femratio +  uncodified+ business + humanities  +  numeracy  + social+  team  +ICT + as.factor(COUNTRY)
```

### Lasso Regression

```{r}
set.seed(123)
lasso_grid <- expand.grid(fraction = seq(.01, 1, length = 100))
lasso_tune <- train(formula2, data = training,
                    method='lasso',
                    preProc=c('scale','center'),
                    tuneGrid = lasso_grid,
                    trControl=ctrl)
```

```{r}
set.seed(123)
test_results$lasso <- round(predict(lasso_tune, testing),8)
postResample(pred = test_results$lasso,  obs = test_results$iscoprop)
```

## MACHINE LEARNING

### K-Nearest Neighbors (KNN)

```{r}
set.seed(123)
knn_tune <- train(formula2, 
                  data = training,
                  method = "kknn",   
                  preProc=c('scale','center'),
                  tuneGrid = data.frame(kmax=4:10,distance=2,kernel='optimal'),
                  trControl = ctrl)
```

```{r}
set.seed(123)
test_results$knn <- round(predict(knn_tune, testing),8)
postResample(pred = test_results$knn,  obs = test_results$iscoprop)
```

```{r}
set.seed(123)
imp <- varImp(knn_tune, scale = FALSE) 
plot(varImp(knn_tune, scale = F), scales = list(y = list(cex = .95))) 
```


### Random Forest

```{r}
set.seed(123)
rf_tune <- train(formula2, 
                 data = training,
                 method = "rf",
                 preProc=c('scale','center'),
                 trControl = ctrl,
                 ntree = 100,
                 tuneGrid = data.frame(mtry=c(1,3,5,7)),
                 importance = TRUE)

test_results$rf <- round(predict(rf_tune, testing),8)
postResample(pred = test_results$rf,  obs = test_results$iscoprop)
```

```{r}
plot(varImp(rf_tune, scale = F), scales = list(y = list(cex = .95))) 
```

### Gradient Boosting

```{r}
set.seed(123)
control <- trainControl(method = "cv", number = 5)
grid <- expand.grid(interaction.depth = c(1, 3, 5),
                    n.trees = c(50, 100, 150),
                    shrinkage = 0.1,
                    n.minobsinnode = 10)

xgb <- train(formula2, data = training,
               method = "gbm",
               trControl = control,
               tuneGrid = grid,
               verbose = FALSE)

# 6. Evaluate the model's performance
test_results$xgb <- round(predict(xgb, newdata = testing),8)
postResample(pred = test_results$xgb,  obs = test_results$iscoprop)
plot(varImp(xgb, scale = F), scales = list(y = list(cex = .95))) 
```

Let’s summarize the MAE for all the tools to select the best models in predicting our response variable:

```{r}
diff_rounded <- apply(test_results[-1], 2, function(x) round(mean(abs(x - test_results$iscoprop)), digits = 8))
diff_rounded
```

```{r}
# Combination
test_results$pred_change = (test_results$knn + test_results$rf)/2
postResample(pred = test_results$pred_change,  obs = test_results$iscoprop)
```

```{r}
yhat = test_results$pred_change
hist(yhat, col="lightblue")
```

**Comparison of real vs predicted average change in employment levels across occupations**

```{r}
real_change <- test_results %>% ggplot(aes(x=iscoprop)) + 
  geom_density(fill="navyblue") + xlim(0, 20)
pred_change<- test_results |> ggplot(aes(x=pred_change)) + 
  geom_density(fill="navyblue") +  xlim(0, 20)

# Arrange together
grid <- ggarrange(real_change,pred_change,
          ncol =2, nrow=1) 
grid + ggtitle("Comparison of real vs predicted proportion of employees in 2019 across ocupations") 
```


### extra

```{r fig.height=20, fig.width=20}
ggplot(merged_data_subset,
       mapping = aes(x = YEAR, y = avg_year, color = COUNTRY)) +
  geom_point() +
  geom_smooth(method = "loess", se = F) +
  facet_wrap(~ job, ncol = 5) +
  scale_fill_manual() +
  labs(x = "", y = "", color = "Country") 
```
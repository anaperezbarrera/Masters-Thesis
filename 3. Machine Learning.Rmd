---
title: "Master's Thesis Code - Statistical Analysis"
author: "Ana Pérez"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r,eval=FALSE}
sum_emp_isco <-merged_data_subset %>%
  group_by(YEAR, isco) %>%
  summarise(sum_isco = sum(totalemp, na.rm = TRUE)) 

sum_emp_year <- merged_data_subset %>%
  group_by(YEAR) %>%
  summarise(sum_year = sum(totalemp, na.rm = TRUE))

merged_totals <- sum_emp_isco%>%
  left_join(sum_emp_year, by = c("YEAR")) 

merged_totals <- merged_totals %>%
  mutate(share = (sum_isco / sum_year) * 100) |> dplyr::select(YEAR,isco,share)

 merged_totals <- merged_totals |> 
  arrange(isco, YEAR) %>% 
   group_by(isco) |> # Ordenar por isco y YEAR para asegurar el orden correcto
  mutate(cambio = share - lag(share),  # Calcular el cambio
         cambio = ifelse(is.na(cambio), 0, round(cambio, 2))) 

merged_data_subset<- merged_data_subset%>%
  left_join(merged_totals, by = c("YEAR","isco"))
```

## PREPARING THE DATA 

```{r}
library(lme4)
library(lmerTest)
library(modelsummary) 
library(broom)
library(knitr)
library(sjPlot)
library(kableExtra)
```

### 1) Proportion of employees 

### NA imputation

```{r}
new_data <- merged_data_subset |> 
  dplyr::select(-job,-sector,-occup,-totalemp,-empchange,-avg_change) |> 
  distinct(.keep_all = TRUE)
  
sapply(new_data, function(x) sum(is.na(x))*100/nrow(new_data))
```

We have to make sure there are no NA'S in our data. For that purpose, and since the number of NA is very low, we are using the...imputations with the Random Forest algorithm from the mice function. 

```{r}
set.seed(123)
m = 5
mice_mod <- mice(new_data, m=m, method='rf')
new_data <- complete(mice_mod, action=m)
```

### Distribution of target variable

This is the distribution of the outcome variable: 

```{r}
new_data %>% 
  ggplot(aes(x=iscoprop)) +
  geom_density(fill="navyblue") + theme_minimal()
```

```{r}
training <- subset(new_data, YEAR >= 2012 & YEAR <= 2018) 
testing<- subset(new_data, YEAR ==2019) 
```

### Correlations

```{r}
cor_data <-training |> dplyr::select(-COUNTRY,-YEAR,-isco)
corr_change <- sort(cor(cor_data)["iscoprop",], decreasing = TRUE)
corr <- data.frame(Predictor = names(corr_change), Correlation = corr_change)
```

```{r}
nudge_x <- 0.01
my_col <- viridisLite::mako(4)[2]

corr %>% 
  ggplot(aes(y = reorder(Predictor, Correlation), x = Correlation)) +
  geom_point(col = my_col) +
  geom_segment(aes(xend = 0, yend = Predictor), col = my_col, size = 0.75) +
  geom_text(
    aes(x = 0, label = Predictor),
    size = 2.5,
    hjust = if_else(corr$Correlation > 0, 1, 0),
    nudge_x = if_else(corr$Correlation > 0, -nudge_x, nudge_x)
  ) +
  scale_y_discrete(breaks = NULL) +
  labs(title = "Correlations with the target variable",
       x = '', y = element_blank()) + theme_minimal()
```

### REGRESSION METHODS

#### Lambda coefficients

```{r}
set.seed(123)
# Use non-numeric
X <- cor_data |> select (-iscoprop)
Y <- cor_data |> select (iscoprop)

grid = 10^seq(10, -2, length = 100)

lasso_data<-cv.glmnet(x=as.matrix(X), y=Y[,1], lambda=grid, alpha=1)
lambda_min<-predict(lasso_data, newx=as.matrix(X)[1:10,], s="lambda.min")
lasso_coeff <- as.data.frame(as.matrix(predict(lasso_data, type="coefficients", s="lambda.min"))) |>  mutate(across(c(lambda.min), round, 2))

coeff_df <- as.data.frame(as.matrix(lasso_coeff))
coeff_df$variable <- rownames(coeff_df)

# Filter non-zero coefficients and exclude intercept
selected_variables <- coeff_df %>%
  filter(lambda.min != 0) %>%
  pull(variable) %>%
  setdiff("(Intercept)")

print(selected_variables)
```
#### Ordinary Least Squares (OLS)

```{r}
# Subset the original data to include only the selected variables
formula <- as.formula(paste("iscoprop ~ ", paste(selected_variables, collapse = " + ")))
```

```{r}
set.seed(123)
# Fit a linear model
lm <- lm(formula, data = selected_data)
summary(lm)
```
#### OLS controlling by country and time-fixed effects

```{r}
all_variables <- c(selected_variables, "as.factor(COUNTRY)", "YEAR")
formula <- as.formula(paste("iscoprop ~ ", paste(all_variables, collapse = " + ")))
selected_data <- training %>% select(all_of(c("iscoprop", selected_variables, "COUNTRY","YEAR")))
```


```{r}
set.seed(123)
# Fit a linear model
lm2 <- lm(formula, data = selected_data)
summary(lm2)
```

#### Linear Mixed Models

```{r}
set.seed(123)
lmm <- lmer(iscoprop ~  sunratio + nightratio + shiftratio +
    supratio + femratio +strength + dexterity + 
    navigation + uncodified + business + humanities + numeracy + 
    infogath + creativity + planning + social + serving + managing + 
    autonomy + latitude + control + team + repetitiv + standard + 
    certainty + machines + ICT + ICTbasic + ICTadvanced  + 
      (1 | COUNTRY) +  (1 | YEAR), data = training)

summary(lmm)
```

```{r}
lmm2 <- lmer(iscoprop ~  strength + dexterity + 
    navigation + uncodified + business + humanities + numeracy + 
    infogath + creativity + planning + social + serving + managing + 
    autonomy + latitude + control + team + repetitiv + standard + 
    certainty + machines + ICT + ICTbasic + ICTadvanced +
           (1 + sunratio + nightratio + shiftratio +
    supratio + femratio | COUNTRY) + 
           (1 | YEAR), 
          control = lmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 10000)), data = training)

summary(lmm2)
```

```{r}
random_effects <- ranef(lmm2)

# Random intercepts for COUNTRY
random_intercepts_country <- random_effects$COUNTRY

# Convert to data frame
random_intercepts_df <- as.data.frame(random_intercepts_country)


```


### Evaluation and Robustness

From now on, we are going to try many models, so it’s convenient to create a data frame with all the predictors.

```{r}
test_results <- data.frame(iscoprop = testing$iscoprop)
```

```{r}
# Linear model 
test_results$ols <- predict(lm, newdata = testing, allow.new.levels = TRUE)
lm<- postResample(pred = test_results$ols,  obs = test_results$iscoprop)

test_results$ols2 <- predict(lm2, newdata = testing, allow.new.levels = TRUE)
lm2<- postResample(pred = test_results$ols2,  obs = test_results$iscoprop)

# Linear Mixed Model 
test_results$lmm <- predict(lmm, newdata = testing, allow.new.levels = TRUE)
lmm<-postResample(pred = test_results$lmm,  obs = test_results$iscoprop)

test_results$lmm2 <- predict(lmm2, newdata = testing, allow.new.levels = TRUE)
lmm2<-postResample(pred = test_results$lmm2,  obs = test_results$iscoprop)
```

```{r}
metrics_table <- data.frame(
  Model = c("OLS (1)","OLS(2)", "LMM (1)", "LMM (2)"),
  RMSE = c(lm[1], lm2[1],lmm[1], lmm2[1]),
  R2 = c(lm[2], lm2[2],lmm[2], lmm2[2]),
  MAE = c(lm[3], lm2[3],lmm[3], lmm2[3])
)
metrics_table
```

```{r}
qplot(test_results$lmm2, test_results$iscoprop) + 
  labs(title="Linear Regression Observed VS Predicted",
       x="Predicted", y="Observed") +
  lims(x = c(0,20), y = c(0,20)) +
  geom_abline(intercept = 0, slope = 1, colour = "darkred") +
  theme_bw()
```


```{r}
coefs <- summary(lmm2)$coefficients

coefs_df <- data.frame(Variable = rownames(coefs), Estimate = coefs[, 1], CI_low = coefs[, 1] - 1.96 * coefs[, 2], CI_high = coefs[, 1] + 1.96 * coefs[, 2])

ggplot(coefs_df, aes(x = reorder(Variable, Estimate), y = Estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = CI_low, ymax = CI_high), width = 0.2) +
  coord_flip() +
  theme_minimal() 
```

Let’s summarize the MAE for all the tools to select the best models in predicting our response variable:

```{r}
diff_rounded <- apply(test_results[-1], 2, function(x) round(mean(abs(x - test_results$iscoprop)), digits = 8))
diff_rounded
```

```{r}
# Combination
test_results$pred_change = test_results$lmm2
postResample(pred = test_results$pred_change,  obs = test_results$iscoprop)
```

```{r}
yhat = test_results$pred_change
hist(yhat, col="lightblue")
```

**Comparison of real vs predicted average change in employment levels across occupations**

```{r}
real_change <- test_results %>% ggplot(aes(x=iscoprop)) + 
  geom_density(fill="navyblue") + xlim(0, 20) 
pred_change<- test_results |> ggplot(aes(x=pred_change)) + 
  geom_density(fill="navyblue") +  xlim(0, 20)

# Arrange together
ggarrange(real_change,pred_change,
          ncol =2, nrow=1) 
```


```{r}
library(sjPlot)
#plot_model(model3, type = "pred")

fixed_effects <- fixef(lmm2)
importance <- abs(fixed_effects[-1])  # Exclude the intercept
importance_df <- data.frame(Variable = names(importance), Importance = importance)
importance_df <- importance_df %>% arrange(desc(Importance))

top_effects <- importance_df$Variable[1:5]

plot <- plot_model(lmm2, type = "est", show.values = TRUE, value.offset = .3) +
  labs(title = "Fixed Effects - model3") + theme_minimal()
  
plot +
  scale_x_discrete(limits = top_effects) + labs(x="",y="",title = "")
```

```{r}
# Custom train function to fit lmer model and return relevant metrics
train_lmer <- function(data, formula, trControl) {
  trainIndex <- createResample(data$iscoprop, times = trControl$number)
  results <- lapply(trainIndex, function(indices) {
    trainData <- data[indices, ]
    testData <- data[-indices, ]
    model <- lmer(formula, data = trainData)
    predictions <- predict(model, newdata = testData, allow.new.levels = TRUE)
    data.frame(obs = testData$iscoprop, pred = predictions)
  })
  
  # Combine results and calculate performance metrics
  all_results <- do.call(rbind, results)
  mae <- mean(abs(all_results$obs - all_results$pred))
  mse <- mean((all_results$obs - all_results$pred)^2)
  rmse <- sqrt(mse)
  r_squared <- cor(all_results$obs, all_results$pred)^2
  
  list(mae = mae, mse = mse, rmse = rmse, r_squared = r_squared)
}

# Define the trainControl for cross-validation
train_control <- trainControl(method = "cv", number = 5, verboseIter = TRUE)

# Define the model formula
model_formula <- iscoprop ~ strength + dexterity + navigation + uncodified + 
    business + humanities + numeracy + infogath + creativity + planning + 
    social + serving + managing + autonomy + latitude + control + team + 
    repetitiv + standard + certainty + machines + ICT + ICTbasic + 
    ICTadvanced + (1 + sunratio + nightratio + shiftratio + supratio + 
    femratio | COUNTRY) + (1 | YEAR)

# Train and evaluate the model using custom function
results <- train_lmer(training, model_formula, train_control)

# Print the performance metrics
print(results)
```



